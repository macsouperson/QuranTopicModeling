{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Author: Omar Masood\n",
    "# Class: ISYE6740\n",
    "# Final Project\n",
    "# Title: Topic Modeling of Religious Texts - Measuring Coherence and Performance. \n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bertopic import BERTopic\n",
    "import gensim.utils as gu \n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from top2vec import Top2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model as ldavis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import umap_ as UMAP\n",
    "import umap\n",
    "from hdbscan import HDBSCAN\n",
    "import os\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "#import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return each verse with the words in their lemmetized form.\n",
    "def clean_docs(data):\n",
    "    lem_text = []\n",
    "    items = []\n",
    "\n",
    "    cln_txt = [t.lemma_ for t in nlp(data) if (t not in sw) and (len(t)>=3)]\n",
    "    lem = \" \".join(cln_txt)\n",
    "    cln = gu.simple_preprocess(lem, deacc=True)\n",
    "    \n",
    "    cleaned = \" \".join(cln)\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_topics(lda_model, vector):\n",
    "    terms = vector.get_feature_names_out()\n",
    "\n",
    "    for i, comp in enumerate(lda_model.components_):\n",
    "        vocab_dic = zip(terms, comp)\n",
    "        sorted_words = sorted(vocab_dic, key=lambda x:x[1], reverse=True)[:20]\n",
    "        print(\"Topic \", i)\n",
    "        for t in sorted_words: print(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWords(data):\n",
    "    words = []\n",
    "\n",
    "    words = [gu.simple_preprocess(i, deacc=True) for i in data]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTopics(lda_model, vector):\n",
    "    features = vector.get_feature_names_out()\n",
    "\n",
    "    topics = []\n",
    "    weights = []\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda_model.components_):\n",
    "        top_features_ind = topic.argsort()[-20:]\n",
    "        topics.append(features[top_features_ind])\n",
    "        weights.append(topic[top_features_ind])\n",
    "    \n",
    "    return topics, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDATopicDiveristy(lda_model, vector):\n",
    "    topics = getTopics(lda_model=lda_model, vector=vector)\n",
    "    output = {'topics':topics}\n",
    "    \n",
    "    score = TopicDiversity(topk=5)\n",
    "    \n",
    "    return score.score(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function just get the pieces ready that are needed to calculate the topic coherence score for our LDA models. \n",
    "def setupCoherence(data, lda_model, vector):\n",
    "\n",
    "    token_docs = [gu.simple_preprocess(d, deacc=True) for d in data]\n",
    "    dictionary = corpora.Dictionary(token_docs)\n",
    "    corpus = [dictionary.doc2bow(t) for t in token_docs]\n",
    "    topics = getTopics(lda_model, vector)\n",
    "\n",
    "    return token_docs, dictionary, corpus, topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate LDA topic coherence score using gensim package\n",
    "def getCoherenceScore(data, lda_model, vector):\n",
    "    token_docs, dictionary, corpus, topics = setupCoherence(data, lda_model, vector)\n",
    "\n",
    "    cm = CoherenceModel(topics=topics, texts=token_docs, \n",
    "                        dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return cm.get_coherence()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a wrapper function so that I can use the gensim topic coherence score in my RandomSearchCV\n",
    "def scorer(lda_model, data):\n",
    "    sw = stopwords.words('english')\n",
    "    vector = TfidfVectorizer(stop_words=sw, max_features=1000)\n",
    "    vect_text = vector.fit_transform(data)\n",
    "\n",
    "    lda = clone(lda_model)\n",
    "    topics = lda.fit_transform(vect_text)\n",
    "    \n",
    "    feature_names = vector.get_feature_names_out()\n",
    "\n",
    "    token_docs, dictionary, corpus, topics = setupCoherence(data, lda_model, vector)\n",
    "    cm = CoherenceModel(topics=topics, texts=token_docs, dictionary=dictionary, coherence='c_v')\n",
    "\n",
    "    return cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Borrowed code from here to calculate BERTopic topic coherence score, https://github.com/MaartenGr/BERTopic/issues/90\n",
    "def BERTCoherence(brt_model, data, topics):\n",
    "    vectorizer = brt_model.vectorizer_model\n",
    "    analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "    words = vectorizer.get_feature_names_out\n",
    "    tokens = [analyzer(doc) for doc in data]\n",
    "\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [[words for words, _ in brt_model.get_topic(topic)] \n",
    "                   for topic in range(len(set(topics))-1)]\n",
    "    \n",
    "    coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                        texts=tokens, \n",
    "                                        corpus=corpus,\n",
    "                                        dictionary=dictionary, \n",
    "                                        coherence='c_v')\n",
    "    \n",
    "    coherence = coherence_model.get_coherence()\n",
    "\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupLDA(vect_text):\n",
    "\n",
    "    lda_model = LatentDirichletAllocation(learning_method='online')\n",
    "    topics = lda_model.fit_transform(vect_text)\n",
    "\n",
    "    return lda_model, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I borrowed the code from here: https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(30, 15), sharex=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_features_ind = topic.argsort()[-n_top_words:]\n",
    "            top_features = feature_names[top_features_ind]\n",
    "            weights = topic[top_features_ind]\n",
    "\n",
    "            ax = axes[topic_idx]\n",
    "            ax.barh(top_features, weights, height=0.7)\n",
    "            ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "            ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "            for i in \"top right left\".split():\n",
    "                ax.spines[i].set_visible(False)\n",
    "            fig.suptitle(title, fontsize=40)\n",
    "\n",
    "        plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Top2VecCoherence(data, topics):\n",
    "    doc_tokens = [d.split() for d in data]\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,3),\n",
    "                                 tokenizer=lambda x: x, \n",
    "                                 preprocessor=lambda x: x,\n",
    "                                 max_df=0.9)\n",
    "    \n",
    "    X = vectorizer.fit_transform(doc_tokens)\n",
    "\n",
    "    topic_word_lists = [list(words[:20]) for words in topics]\n",
    "\n",
    "    dictionary = corpora.Dictionary(doc_tokens)\n",
    "\n",
    "    coh_model = CoherenceModel(topics=topic_word_lists, \n",
    "                               texts=doc_tokens, \n",
    "                               dictionary=dictionary, \n",
    "                               coherence='c_v')\n",
    "\n",
    "    return coh_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParams(lda_model, vect_text):\n",
    "    batch = range(2000,10000,2000)\n",
    "    learning = [1,10,50,100,500,1000]\n",
    "    decay = [0.6,0.7,0.75,0.8]\n",
    "    doc_top = [0.001, 0.01, 0.1, 1]\n",
    "    n_comp = [1,2,3,4,5,7,10,15,20,25]\n",
    "\n",
    "    parameters = {'n_components':n_comp,\n",
    "                'doc_topic_prior':doc_top,\n",
    "                'learning_decay':decay,\n",
    "                'learning_offset':learning,\n",
    "                'batch_size':batch}\n",
    "\n",
    "    clf = RandomizedSearchCV(lda_model, param_distributions=parameters, cv=5, random_state=6740, scoring=scorer)\n",
    "    search = clf.fit(vect_text)\n",
    "    return search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the topic diversity of the BERTopic model\n",
    "def calculateTopicDiversity(brt_model):\n",
    "    n = len(brt_model.get_topics())\n",
    "\n",
    "    topic_list = []\n",
    "    for i in range(n-1):\n",
    "        topic = brt_model.get_topic(i)\n",
    "        words_list = [t[0].replace(\" \", \"_\") for t in topic]\n",
    "        topic_list.append(words_list)\n",
    "\n",
    "    output = {'topics':topic_list}\n",
    "    score = TopicDiversity(topk=5)\n",
    "    return score.score(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the topic diversity of the Top2Vec model\n",
    "def Top2VecTopicDiversity(topics):\n",
    "    word_list = []\n",
    "    \n",
    "    for t in topics:\n",
    "        test = [w.replace(\" \",\"_\") for w in t]\n",
    "        word_list.append(test)\n",
    "\n",
    "    output = {'topics':word_list}\n",
    "    score = TopicDiversity(topk=5)\n",
    "\n",
    "    return score.score(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##I needed this to download some stuff for the nltk package\n",
    "# os.environ['NTLK_DATA'] = \"Users/xxxxxx/Desktop/Spring 20XX/??????/Project/nltk_data/\"\n",
    "# nltk.data.path.append(\"Users/xxxxxx/Desktop/Spring 20XX/?????/Project/nltk_data/\")\n",
    "# try:\n",
    "#     _create_unverified_https_context = ssl._create_unverified_context\n",
    "# except AttributeError:\n",
    "#     pass\n",
    "# else:\n",
    "#     ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(6740)\n",
    "\n",
    "files = ['data/quran.csv', 'data/oldtestament.csv', 'data/newtesament.csv']\n",
    "texts = [pd.read_csv(f) for f in files]\n",
    "\n",
    "quran = texts[0]\n",
    "oldt = texts[1]\n",
    "newt = texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_quran = quran.ayah_en\n",
    "test_oldt = oldt.text\n",
    "test_newt = newt.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TfidfVectorizer(stop_words=sw, max_features=1000,\n",
    "                             ngram_range=(1,3))\n",
    "\n",
    "clean_quran = test_quran.map(clean_docs)\n",
    "qur_vec_text = vector.fit_transform(clean_quran)\n",
    "\n",
    "clean_old_testament = test_oldt.map(clean_docs)\n",
    "oldt_vec_text = vector.fit_transform(clean_old_testament)\n",
    "\n",
    "clean_new_testament = test_newt.map(clean_docs)\n",
    "newt_vec_text = vector.fit_transform(clean_new_testament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Commenting this part out - used RandomSearchCV to find the best parameters for our LDA model\n",
    "\n",
    "#qur_best_params = getBestParams(new_lda_model, qur_vec_text)\n",
    "# oldt_best_params = getBestParams(olt_lda_model, oldt_vec_text)\n",
    "# # newt_best_params = getBestParams(newt_lda_model, newt_vec_text)\n",
    "# # {'n_components': 7,\n",
    "# #  'learning_offset': 500,\n",
    "# #  'learning_decay': 0.7,\n",
    "# #  'doc_topic_prior': 1,\n",
    "# #  'batch_size': 4000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best parameters for LDA model after tuning\n",
    "new_lda_model = LatentDirichletAllocation(learning_method='online',learning_offset=500, n_components=7,\n",
    "                                      learning_decay=0.7, doc_topic_prior=1, batch_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "The scores for this LDA mode are as follows, Coherence: 0.508835159600508, Topic Diversity: 0.8, and perplexity: 2407.5811181354275\n"
     ]
    }
   ],
   "source": [
    "qur_topics = new_lda_model.fit_transform(qur_vec_text)\n",
    "topic_div = LDATopicDiveristy(new_lda_model, vector)\n",
    "coherence = getCoherenceScore(clean_quran,new_lda_model,vector)\n",
    "perplexity = new_lda_model.perplexity(qur_vec_text)\n",
    "\n",
    "print(f'The scores for this LDA mode are as follows, Coherence: {coherence}, Topic Diversity: {topic_div}, and perplexity: {perplexity}')\n",
    "#The scores for this LDA mode are as follows, Coherence: 0.39811689145911894, Topic Diversity: 0.6571428571428571, and perplexity: 2406.5657013769955"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "The scores for this LDA mode are as follows, Coherence: 0.5256238686035462, Topic Diversity: 0.6571428571428571, and perplexity: 1220.9917771332357\n"
     ]
    }
   ],
   "source": [
    "oldt_topic = new_lda_model.fit_transform(oldt_vec_text)\n",
    "topic_div = LDATopicDiveristy(new_lda_model, vector)\n",
    "coherence = getCoherenceScore(clean_old_testament,new_lda_model,vector)\n",
    "perplexity = new_lda_model.perplexity(oldt_vec_text)\n",
    "\n",
    "print(f'The scores for this LDA mode are as follows, Coherence: {coherence}, Topic Diversity: {topic_div}, and perplexity: {perplexity}')\n",
    "#The scores for this LDA mode are as follows, Coherence: 0.38077568084480656, Topic Diversity: 0.5714285714285714, and perplexity: 1221.0900062361586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "The scores for this LDA mode are as follows, Coherence: 0.3029100953841114, Topic Diversity: 0.6571428571428571, and perplexity: 1940.52763345102\n"
     ]
    }
   ],
   "source": [
    "newtt_topic = new_lda_model.fit_transform(newt_vec_text)\n",
    "topic_div = LDATopicDiveristy(new_lda_model, vector)\n",
    "coherence = getCoherenceScore(clean_new_testament,new_lda_model,vector)\n",
    "perplexity = new_lda_model.perplexity(newt_vec_text)\n",
    "\n",
    "print(f'The scores for this LDA mode are as follows, Coherence: {coherence}, Topic Diversity: {topic_div}, and perplexity: {perplexity}')\n",
    "#The scores for this LDA mode are as follows, Coherence: 0.30797002263842777, Topic Diversity: 0.6, and perplexity: 1940.0431470305832"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_feature_names = vector.get_feature_names_out()\n",
    "plot_top_words(new_lda_model, tf_feature_names, 10, \"Topics in LDA model\")\n",
    "# topics, weights1 = getTopics(new_lda_model, qur_vec)\n",
    "# print(weights1)\n",
    "# print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_feature_names = oldt_vec.get_feature_names_out()\n",
    "# plot_top_words(new_lda_model, tf_feature_names, 15, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_feature_names = newt_vec.get_feature_names_out()\n",
    "# plot_top_words(new_lda_model, tf_feature_names, 15, \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_data = ldavis.prepare(new_lda_model, qur_vec_text, qur_vec)\n",
    "#prep_data = ldavis.prepare(new_lda_model, oldt_vec_text, oldt_vec)\n",
    "# pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.display(prep_data)\n",
    "\n",
    "# pyLDAvis.save_html(prep_data,'QuranLDA.html')\n",
    "# prep_data = ldavis.prepare(new_lda_model, oldt_vec_text, oldt_vec)\n",
    "# pyLDAvis.save_html(prep_data,'OLDLDA.html')\n",
    "# prep_data = ldavis.prepare(new_lda_model, newt_vec_text, newt_vec)\n",
    "# pyLDAvis.save_html(prep_data,'NEWLDA.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "##BERTopic\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=True)\n",
    "umap_model = umap.UMAP(n_neighbors=15, n_components=10, metric='cosine', low_memory=False)\n",
    "tuned_brt_model = BERTopic(language='english', embedding_model='all-MiniLM-L6-v2', n_gram_range=(1,3), \n",
    "                          umap_model=umap_model, hdbscan_model=hdbscan_model, top_n_words=5, nr_topics=\"auto\")\n",
    "\n",
    "# brt_model = BERTopic(language='english', embedding_model='all-MiniLM-L6-v2', n_gram_range=(1,2), \n",
    "#                           umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "\n",
    "# n_words = range(5,30,5)\n",
    "# topic_size = range(100,500,50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 topics and the Coherence for the BERT Model is 0.7800520232897611 and the Topic Diversity is 0.963265306122449\n"
     ]
    }
   ],
   "source": [
    "qur_tops, probs = tuned_brt_model.fit_transform(clean_quran)\n",
    "coherence = BERTCoherence(tuned_brt_model, clean_quran, qur_tops)\n",
    "topic_diversity = calculateTopicDiversity(tuned_brt_model)\n",
    "\n",
    "topics_n = range(1,9)\n",
    "n = len(tuned_brt_model.get_topics())\n",
    "k = list(range(1,n-1))\n",
    "\n",
    "tuned_brt_model.visualize_topics(topics=k).write_html(\"Visualizations/quran_intertopic_dist_map.html\")\n",
    "tuned_brt_model.visualize_barchart(topics=topics_n).write_html(\"Visualizations/quran_barchart.html\")\n",
    "\n",
    "print(f'There are {n} topics and the Coherence for the BERT Model is {coherence} and the Topic Diversity is {topic_diversity}')\n",
    "#There are 109 topics and the Coherence for the BERT Model is 0.7682063071475818 and the Topic Diversity is 0.9037037037037037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15 topics and the Coherence for the BERT Model is 0.8171662854204647 and the Topic Diversity is 0.9\n"
     ]
    }
   ],
   "source": [
    "oldt_tops, probs = tuned_brt_model.fit_transform(clean_old_testament)\n",
    "coherence = BERTCoherence(tuned_brt_model, clean_old_testament, oldt_tops)\n",
    "topic_diversity = calculateTopicDiversity(tuned_brt_model)\n",
    "\n",
    "n = len(tuned_brt_model.get_topics())\n",
    "k = list(range(1,n-1))\n",
    "\n",
    "tuned_brt_model.visualize_topics(topics=k).write_html(\"Visualizations/old_testament_intertopic_dist_map.html\")\n",
    "tuned_brt_model.visualize_barchart(topics=topics_n).write_html(\"Visualizations/old_testament_barchart.html\")\n",
    "\n",
    "print(f'There are {n} topics and the Coherence for the BERT Model is {coherence} and the Topic Diversity is {topic_diversity}')\n",
    "#There are 259 topics and the Coherence for the BERT Model is 0.8498131720508381 and the Topic Diversity is 0.8806201550387597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 topics and the Coherence for the BERT Model is 0.7980990952545068 and the Topic Diversity is 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "newt_tops, probs = tuned_brt_model.fit_transform(clean_new_testament)\n",
    "coherence = BERTCoherence(tuned_brt_model, clean_new_testament, newt_tops)\n",
    "topic_diversity = calculateTopicDiversity(tuned_brt_model)\n",
    "\n",
    "n = len(tuned_brt_model.get_topics())\n",
    "k = list(range(1,n-1))\n",
    "\n",
    "tuned_brt_model.visualize_topics(topics=k).write_html(\"Visualizations/new_testament_intertopic_dist_map.html\")\n",
    "tuned_brt_model.visualize_barchart(topics=topics_n).write_html(\"Visualizations/new_testament_barchart.html\")\n",
    "\n",
    "print(f'There are {n} topics and the Coherence for the BERT Model is {coherence} and the Topic Diversity is {topic_diversity}')\n",
    "#There are 105 topics and the Coherence for the BERT Model is 0.7924715016683221 and the Topic Diversity is 0.9442307692307692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in n_words:\n",
    "#     for k in topic_size:\n",
    "#         test_brt_model = BERTopic(language='english', embedding_model='all-MiniLM-L6-v2', n_gram_range=(1,2), \n",
    "#                           umap_model=umap_model, hdbscan_model=hdbscan_model, top_n_words=i, min_topic_size=k)\n",
    "#         test_brt_model.fit_transform(clean_quran)\n",
    "#         coherence = BERTCoh(test_brt_model, clean_quran)\n",
    "#         print(\"For top_n_words: \"+str(i)+\" and min_topic_size: \"+str(k)+\" the coherence is: \"+str(coherence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:01:55,512 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-04-27 15:01:55,621 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-04-27 15:02:30,312 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-04-27 15:02:32,035 - top2vec - INFO - Finding dense areas of documents\n",
      "2025-04-27 15:02:32,292 - top2vec - INFO - Finding topics\n",
      "2025-04-27 15:02:32,321 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-04-27 15:02:32,860 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-04-27 15:04:58,281 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-04-27 15:05:01,427 - top2vec - INFO - Finding dense areas of documents\n",
      "2025-04-27 15:05:02,669 - top2vec - INFO - Finding topics\n",
      "2025-04-27 15:05:02,806 - top2vec - INFO - Pre-processing documents for training\n",
      "2025-04-27 15:05:03,035 - top2vec - INFO - Creating joint document/word embedding\n",
      "2025-04-27 15:05:46,696 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2025-04-27 15:05:48,475 - top2vec - INFO - Finding dense areas of documents\n",
      "2025-04-27 15:05:48,863 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "hdbscan_params = {'min_cluster_size':10, 'metric':'euclidean', 'prediction_data':True, }\n",
    "umap_params = {'n_neighbors':15, 'n_components':10, 'metric':'cosine', 'low_memory':False}\n",
    "\n",
    "qur_t2v_model = Top2Vec(clean_quran,embedding_model='doc2vec', speed='deep-learn', \n",
    "                         min_count=25, hdbscan_args=hdbscan_params, umap_args=umap_params)\n",
    "\n",
    "oldt_t2v_model = Top2Vec(clean_old_testament,embedding_model='doc2vec', speed='deep-learn',  \n",
    "                         min_count=25, hdbscan_args=hdbscan_params, umap_args=umap_params)\n",
    "\n",
    "newt_t2v_model = Top2Vec(clean_new_testament,embedding_model='doc2vec', speed='deep-learn', \n",
    "                         min_count=25, hdbscan_args=hdbscan_params, umap_args=umap_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reducing topics: 100%|██████████| 85/85 [00:00<00:00, 280.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this Top2Vec model the Topic Diversity is 0.808, coherence is 0.38736377901991825\n"
     ]
    }
   ],
   "source": [
    "qur_t2v_model.hierarchical_topic_reduction(50)\n",
    "topics = qur_t2v_model.topic_words_reduced\n",
    "td = Top2VecTopicDiversity(topics)\n",
    "c = Top2VecCoherence(clean_quran, topics)\n",
    "\n",
    "print(f'For this Top2Vec model the Topic Diversity is {td}, coherence is {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reducing topics: 100%|██████████| 356/356 [00:14<00:00, 25.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this Top2Vec model the Topic Diversity is 0.9466666666666667, coherence is 0.5222067723212561\n"
     ]
    }
   ],
   "source": [
    "oldt_t2v_model.hierarchical_topic_reduction(30)\n",
    "topics = oldt_t2v_model.topic_words_reduced\n",
    "td = Top2VecTopicDiversity(topics)\n",
    "c = Top2VecCoherence(clean_old_testament, topics)\n",
    "\n",
    "print(f'For this Top2Vec model the Topic Diversity is {td}, coherence is {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reducing topics: 100%|██████████| 92/92 [00:00<00:00, 192.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this Top2Vec model the Topic Diversity is 0.8857142857142857, coherence is 0.41965162062590566\n"
     ]
    }
   ],
   "source": [
    "newt_t2v_model.hierarchical_topic_reduction(35)\n",
    "topics = newt_t2v_model.topic_words_reduced\n",
    "td = Top2VecTopicDiversity(topics)\n",
    "c = Top2VecCoherence(clean_new_testament, topics)\n",
    "\n",
    "print(f'For this Top2Vec model the Topic Diversity is {td}, coherence is {c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_args = {\"n_neighbors\": 15, \"n_components\": 2,\"metric\": \"cosine\", }\n",
    "umap_model = umap.UMAP(**umap_args).fit_transform(qur_t2v_model.document_vectors)\n",
    "\n",
    "# binary_top = np.array([gettopthree(t) for t in qur_t2v_model.doc_top])\n",
    "# umap.plot.points(umap_model, labels=qur_t2v_model.doc_top)\n",
    "\n",
    "x,y = umap_model[:,0], umap_model[:,1]\n",
    "plt.scatter(x,y,c=qur_t2v_model.doc_top_reduced, \n",
    "            cmap='terrain', marker=\".\", s=8)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('black')\n",
    "plt.title(\"Top2Vec Model Quran (UMAP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(**umap_args).fit_transform(oldt_t2v_model.document_vectors)\n",
    "\n",
    "#binary_top = np.array([gettopthree(t) for t in oldt_t2v_model.doc_top])\n",
    "#umap.plot.points(umap_model, labels=binary_top, theme='fire')\n",
    "\n",
    "x,y = umap_model[:,0], umap_model[:,1]\n",
    "plt.scatter(x,y,c=oldt_t2v_model.doc_top_reduced, \n",
    "            cmap='bwr', marker=\".\", s=8)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('black')\n",
    "plt.title(\"Top2Vec Model Old Testament (UMAP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = umap.UMAP(**umap_args).fit_transform(newt_t2v_model.document_vectors)\n",
    "\n",
    "#binary_top = np.array([gettopthree(t) for t in newt_t2v_model.doc_top])\n",
    "#umap.plot.points(umap_model, labels=binary_top, theme='fire')\n",
    "\n",
    "x,y = umap_model[:,0], umap_model[:,1]\n",
    "plt.scatter(x,y,c=newt_t2v_model.doc_top_reduced, \n",
    "            cmap='Spectral', marker=\".\", s=8)\n",
    "ax = plt.gca()\n",
    "ax.set_facecolor('black')\n",
    "plt.title(\"Top2Vec Model New Testament (UMAP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', prediction_data=False)\n",
    "# umap_model = umap.UMAP(n_neighbors=15, n_components=10, metric='cosine', low_memory=False)\n",
    "# quran_brt_model = BERTopic(language='english', embedding_model='all-MiniLM-L6-v2', n_gram_range=(1,3), \n",
    "#                           umap_model=umap_model, hdbscan_model=hdbscan_model, top_n_words=5, nr_topics=\"auto\")\n",
    "# oldt_brt_model = BERTopic(language='english', embedding_model='all-MiniLM-L6-v2', n_gram_range=(1,3), \n",
    "#                           umap_model=umap_model, hdbscan_model=hdbscan_model, top_n_words=5, nr_topics=\"auto\")\n",
    "# newt_brt_model = BERTopic(language='english', embedding_model='all-MiniLM-L6-v2', n_gram_range=(1,3), \n",
    "#                           umap_model=umap_model, hdbscan_model=hdbscan_model, top_n_words=5, nr_topics=\"auto\")\n",
    "\n",
    "# qur_tops, probs = quran_brt_model.fit_transform(clean_quran)\n",
    "# oldt_top, _ = oldt_brt_model.fit_transform(clean_old_testament)\n",
    "# newt_tops,_ = newt_brt_model.fit_transform(clean_new_testament)\n",
    "\n",
    "# q_o_matrix = cosine_similarity(quran_brt_model.topic_embeddings_, \n",
    "#                                oldt_brt_model.topic_embeddings_) \n",
    "# q_n_matrix = cosine_similarity(quran_brt_model.topic_embeddings_, \n",
    "#                                newt_brt_model.topic_embeddings_)\n",
    "\n",
    "# col1 = []\n",
    "# col2 = []\n",
    "# col3 = []\n",
    "\n",
    "# for i in range(2,12):\n",
    "#     sim_topics1 = np.argmax(q_o_matrix[i])-1\n",
    "#     sim2_topics = np.argmax(q_n_matrix[i])-1\n",
    "#     print(i-1, sim_topics1, sim2_topics)\n",
    "#     col1.append([k[0] for k in quran_brt_model.get_topic(i-1)])\n",
    "#     col2.append([k[0] for k in oldt_brt_model.get_topic(sim_topics1)])\n",
    "#     col3.append([k[0] for k in newt_brt_model.get_topic(sim2_topics)])\n",
    "\n",
    "# table_data = {'quran':col1, 'old_t':col2, 'new_t': col3}\n",
    "# df2 = pd.DataFrame(table_data)\n",
    "# df2.to_latex('BURTtable.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = qur_t2v_model.topic_vectors_reduced\n",
    "y = oldt_t2v_model.topic_vectors_reduced\n",
    "z = newt_t2v_model.topic_vectors_reduced\n",
    "\n",
    "sim_matrix = cosine_similarity(x,y)\n",
    "sim_matrix2 = cosine_similarity(x,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = []\n",
    "col2 = []\n",
    "col3 = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    sim_topics1 = np.argmax(sim_matrix[i])-1\n",
    "    sim2_topics = np.argmax(sim_matrix2[i])-1\n",
    "    print(i, sim_topics1, sim2_topics)\n",
    "    col1.append(qur_t2v_model.topic_words_reduced[i][0:10])\n",
    "    col2.append(oldt_t2v_model.topic_words_reduced[sim_topics1][0:10])\n",
    "    col3.append(newt_t2v_model.topic_words_reduced[sim2_topics][0:10])\n",
    "\n",
    "table_data = {'quran':col1, 'old_t':col2, 'new_t': col3}\n",
    "df2 = pd.DataFrame(table_data)\n",
    "df2.to_latex('t2vtable.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findSimilarTopics(model1, model2, model3):\n",
    "    x = model1.topic_vectors\n",
    "    y = model2.topic_vectors\n",
    "    z = model3.topic_vectors\n",
    "    col1 = []\n",
    "    col2 = []\n",
    "    col3 = []\n",
    "    sim_matrix = cosine_similarity(x,y)\n",
    "    sim_matrix2 = cosine_similarity(x,z)\n",
    "\n",
    "    for i in range(2,12):\n",
    "        sim_topics1 = np.argmax(sim_matrix[i])-1\n",
    "        sim2_topics = np.argmax(sim_matrix2[i])-1\n",
    "        print(i, sim_topics1, sim2_topics)\n",
    "        col1.append(model1.topic_words[i][0:10])\n",
    "        col2.append(model2.topic_words[sim_topics1][0:10])\n",
    "        col3.append(model3.topic_words[sim2_topics][0:10])\n",
    "\n",
    "    table_data = {'model1':col1, 'model2':col2, 'model3': col3}\n",
    "    return pd.DataFrame(table_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
